data:
  train_batch_size: 256
  micro_batch_size: null # will be deprecated, use micro_batch_size_per_gpu
  micro_batch_size_per_gpu: 4  # this is also val batch size
  train_files: ~/data/gsm8k/train.parquet
  val_files: ~/data/gsm8k/test.parquet
  # Single-turn settings
  prompt_key: question
  response_key: answer
  reward_fn_key: data_source
  prompt_dict_keys: null
  response_dict_keys: null
  # Multi-turn settings
  multiturn:
    enable: false  # Set to true to use multi-turn dataset
    messages_key: messages  # Key for messages list in multi-turn mode
    tools_key: tools  # Key for tools list in multi-turn mode
    enable_thinking_key: enable_thinking  # Whether to enable thinking in multi-turn mode
  max_length: 1024
  truncation: error
  balance_dp_token: False
  chat_template: null
  custom_cls:
    path: null
    name: null
  use_shm: False
model:
  partial_pretrain: ~/models/gemma-1.1-7b-it
  use_shm: False
  fsdp_config:
    model_dtype: fp32
    wrap_policy:
      min_num_params: 0
    cpu_offload: False
    offload_params: False
  external_lib: null
  enable_gradient_checkpointing: True
  trust_remote_code: False
  lora_rank: 16  # Set to positive value to enable LoRA (e.g., 32)
  lora_alpha: 32  # LoRA scaling factor
  target_modules: all-linear  # Target modules for LoRA adaptation
  use_liger: False
  strategy: fsdp2
optim:
  lr: 1e-6
  betas: [0.9, 0.95]
  weight_decay: 0.01
  warmup_steps_ratio: 0.01
  clip_grad: 1.0
  lr_scheduler: cosine
ulysses_sequence_parallel_size: 1
use_remove_padding: False
# Rollout model config.
rollout:

  # actor_rollout_ref.rollout.name: hf/vllm/sglang.
  name: vllm

  # sync: LLM, async: AsyncLLM
  mode: sync

  # Sampling temperature for rollout.
  temperature: 1.0

  # Top-k sampling parameter. -1 for vLLM rollout, 0 for HF rollout.
  top_k: -1

  # Top-p sampling parameter. Default 1.0.
  top_p: 1

  # https://arxiv.org/abs/2410.21236
  use_fire_sampling: False

  # typically the same as data max prompt length
  prompt_length: 512

  # typically the same as data max response length
  response_length: 1024

  # for vllm rollout
  # Rollout model parameters type. Align with actor model's FSDP/Megatron type.
  dtype: bfloat16

  # Fraction of GPU memory used by vLLM/SGLang for KV cache.
  gpu_memory_utilization: 0.5

  # Whether to ignore EOS and continue generating after EOS is hit.
  ignore_eos: False

  # Whether to disable CUDA graph. Default True to allow cache freeing.
  enforce_eager: True

  # Whether to free engine KVCache after generation. Set enforce_eager=True when enabled.
  free_cache_engine: True

  # Which loader to use for rollout model weights: dummy_dtensor, hf, megatron, etc.
  # safetensors (for huge model, and set use_shm=True); dummy_dtensor: randomly init model weight
  load_format: dummy_dtensor

  # for huge model, layered summon can save memory (prevent OOM) but make it slower
  layered_summon: False

  # TP size for rollout. Only effective for vLLM.
  tensor_model_parallel_size: 1

  # max number of tokens in a batch
  max_num_batched_tokens: 8192

  # max length for rollout
  max_model_len: null

  # max length of sequences
  max_num_seqs: 1024

  # [Will be deprecated, use log_prob_micro_batch_size_per_gpu] The batch size for one forward pass in the computation of log_prob. Global batch size.
  log_prob_micro_batch_size: null

  # The batch size for one forward pass in the computation of log_prob. Local batch size per GPU.
  log_prob_micro_batch_size_per_gpu: null

  # enable dynamic batch size (sequence packing) for log_prob computation
  log_prob_use_dynamic_bsz: false

  # max token length for log_prob computation
  log_prob_max_token_len_per_gpu: 16384

  # disable logging statistics
  disable_log_stats: True

  # may get higher throughput when set to True. When activated, Please increase max_num_batched_tokens or decrease max_model_len.
  enable_chunked_prefill: True

  # for hf rollout
  # Whether to sample during training rollout. False uses greedy sampling.
  do_sample: True

  # number of responses (i.e. num sample times). > 1 for grpo
  n: 1

  # Extra inference engine arguments (vllm, sglang).
  engine_kwargs:

    # for vllm
    vllm:

      # Swap space (in GB) used by inference engine. null uses default (e.g., 4 GB).
      swap_space: null

      # Whether to disable the preprocessor cache for multimodel models.
      disable_mm_preprocessor_cache: False

    # for sglang
    sglang:

      # The attention backend for sglang engine. Options: flashinfer, triton, flashmla, null for default.
      attention_backend: null

  # Sampling parameters used during validation.
  val_kwargs:

    # sampling parameters for validation
    # Top-k sampling parameter. -1 for vLLM rollout, 0 for HF rollout.
    top_k: -1

    # Top-p sampling parameter. Default 1.0.
    top_p: 0.95

    # Sampling temperature for rollout.
    temperature: 1.0

    # whether to repeat n times for validation
    n: 1

    # Whether to sample during training rollout. False uses greedy sampling.
    do_sample: False

  # Multi-turn interaction config for tools or chat.
  multi_turn:

    # set to True for multi-turn tool interaction tasks; should set rollout.name to sglang as well
    enable: False

    # null for no limit (default max_length // 3)
    max_turns: null

    # null for no tool
    tool_config_path: null

    # null for default callback
    completion_callback: null

    # - When set to True, the model's default chat template is used for multi-turn rollout, which typically matches production behavior.
    # - When set to False, the token ids recorded for training are used instead; unlike the default chat template, these always include the model's full output,
    #   which may contain additional content such as reasoning content. This maintains the consistency between training and rollout, but it will lead to longer prompts.
    use_inference_chat_template: False

    # Tokenization is performed turn by turn and the resulting token ids are concatenated to form the full conversation.
    # To ensure this matches the result of tokenizing the entire conversation at once, a sanity check is run at the end of each multi-turn rollout to compare the two sets of token ids.
    # Some models are known to produce different tokenization results when tokenizing turn by turn vs. all at once. aThis behavior has already been validated for them.
    # To reduce excessive warnings, you can turn off the sanity check for these models if you are using their default chat template:
    # Qwen/QwQ-32B, Qwen/Qwen3-xxB
    enable_tokenization_sanity_check: True

    # Format of the multi-turn interaction. Options: hermes, llama3_json, ...
    format: hermes

  # support logging rollout prob for debugging purpose
  calculate_log_probs: False

  # profiler configs
  profiler:

    # True for each task has its own database, False for all tasks in one training step share one database.
    discrete: False

    # Whether to profile all ranks.
    all_ranks: False

    # The ranks that will be profiled. null or [0,1,...]
    ranks: null
# configs for the reward model
reward_model:

  # Whether to enable reward model. If False, we compute the reward only with the user-defined reward functions.
  # In GSM8K and Math examples, we disable reward model.
  # For RLHF alignment example using full_hh_rlhf, we utilize reward model to assess the responses.
  # If False, the following parameters are not effective
  enable: False

  # FSDP strategy: "fsdp" or "fsdp2"
  #strategy: ${actor_rollout_ref.actor.strategy}

  # model config for reward scoring
  model:

    # Input tokenizer. If the reward model’s chat template is inconsistent with the policy,
    # we need to first decode to plaintext, then apply the rm’s chat_template.
    # Then score with RM. If chat_templates are consistent, it can be set to null.
    #input_tokenizer: ${actor_rollout_ref.model.path}

    # RM’s HDFS path or local path. Note that RM only supports AutoModelForSequenceClassification.
    # Other model types need to define their own RewardModelWorker and pass it from the code.
    path: ~/models/FsfairX-LLaMA3-RM-v0.1

    # Whether to use shared memory for loading the model
    use_shm: False

    # External model implementation (optional)
    #external_lib: ${actor_rollout_ref.model.external_lib}

    # Use remove padding optimization (saves compute)
    use_remove_padding: False

    # Whether to use fused reward kernels for speedup
    #use_fused_kernels: ${actor_rollout_ref.model.use_fused_kernels}

    # Whether to enable loading a remote code model, default to False
    trust_remote_code: False

    # FSDP-specific config
    fsdp_config:

      # Policy for wrapping layers with FSDP
      wrap_policy:

        # Minimum number of parameters to trigger wrapping
        min_num_params: 0

      # Whether to offload model parameters to CPU
      param_offload: False

      # Only for FSDP2: Reshard after forward pass to reduce memory footprint
      reshard_after_forward: True

      # Number of GPUs in each FSDP shard group; -1 means auto
      fsdp_size: -1

      # Only for FSDP1: FSDP1 configuration, prefetch the next forward-pass all-gather
      # before the current forward computation.
      forward_prefetch: False

  # [Deprecated] Global micro batch size
  micro_batch_size: null

  # Local per-GPU micro batch size
  micro_batch_size_per_gpu: null

  # Maximum sequence length to process for scoring
  max_length: null

  # Sequence parallelism size for Ulysses-style model parallelism
  ulysses_sequence_parallel_size: 1

  # Whether to dynamically adjust batch size at runtime
  #use_dynamic_bsz: ${critic.use_dynamic_bsz}

  # Maximum number of tokens per GPU in one forward pass
  #forward_max_token_len_per_gpu: ${critic.forward_max_token_len_per_gpu}

  # Reward Manager. This defines the mechanism of computing rule-based reward and handling different reward sources.
  # Default is naive. If all verification functions are multiprocessing-safe,
  # the reward manager can be set to prime for parallel verification.
  reward_manager: naive

  # Whether to launch custom reward function asynchronously during log_prob
  launch_reward_fn_async: False

  # Cloud/local sandbox fusion configuration for custom reward logic
  sandbox_fusion:

    # Cloud/local function URL for sandbox execution
    url: null

    # Max concurrent requests allowed to sandbox
    max_concurrent: 64

  # profiler configs
  profiler:

    # True for each task has its own database, False for all tasks in one training step share one database.
    discrete: False

    # Whether to profile all ranks.
    all_ranks: False

    # The ranks that will be profiled. null or [0,1,...]
    ranks: null

# custom reward function definition
custom_reward_function:

  # The path to the file containing your customized reward function.
  # If not specified, pre-implemented reward functions will be used.
  path: null

  # The name of the reward function within the specified file. Default is 'compute_score'.
  name: compute_score
trainer:
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
  default_hdfs_dir: null
  resume_path: null
  project_name: gsm8k-sft
  experiment_name: test
  total_epochs: 4
  total_training_steps: null
  logger: [ 'console', 'wandb' ]
  seed: 1

  save_freq: -1
  test_freq: -1
  nnodes: 1
  n_gpus_per_node: 8
  max_ckpt_to_keep: null # TODO
